{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UAT for ReLU\n",
    "## Step 1: function decomposition\n",
    "Let $f(x)$ be a continuous function defined on a compact interval $[a, b]$. <br>\n",
    "By the Stone-Weierstrass Theorem, $f(x)$ can be uniformly approximated by a finite\n",
    "linear combination of basis functions:\n",
    "$$\n",
    "f(x) ≈ \\sum^N_{i=1} c_i ϕ_i(x)\n",
    "$$\n",
    "where $ϕ_i(x)$ are continuous functions and $c_i ∈ R$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: ReLU Approximates Indicator Functions\n",
    "define $g(wx+b) = ReLU(wx+b) - ReLU(wx+b-1)$ <br> <br>\n",
    "then $g(wx+b)$ will be<br>\n",
    "$$\n",
    "g(wx + b) = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "1 & \\text{if } x >(1−b)/w, \\newline\n",
    "wx+b & \\text{if } -b/w < x <(1−b)/w\\newline\n",
    "0 & \\text{if } x < -b/w.\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "A sigmoidal function $g(wx + b)$ can approximate an indicator function:\n",
    "$$χ[x0,x1](x) = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "1 & \\text{if } x ∈ [x0, x1]\\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "For large values of w, the function $g(wx + b)$ transitions sharply at $x = −b/w:$\n",
    "$$\n",
    "g(wx + b) = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "1 & \\text{if } x >−b/w, \\newline\n",
    "0 & \\text{if } x < -b/w.\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Constructing the Neural Network\n",
    "Using a finite sum of ReLU function, we approximate $f(x)$ as:\n",
    "$$\n",
    "\\hat f(x) = \\sum_{i=1}^Nc_i ReLU(w_ix + b_i),\n",
    "$$\n",
    "where $w_i$, $b_i$, and $c_i$ are learnable parameters.<br>\n",
    "\n",
    "P.S:<br>\n",
    "$g(wx-b)$ can be writen as $c_i ReLU(w_ix + b_i) + c_{i+1} ReLU(w_{i+1}x + b_{i+1})$<br>\n",
    "where $c_{i+1} = - c_i$ , $w_{i+1} = w_i$ and $b_{i+1} = b_i - 1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Error Bound\n",
    "Define the approximation error as:<br>\n",
    "$E(x) = |f(x) − ˆf(x)|$.<br>\n",
    "By the uniform continuity of $f(x)$ and the compactness of $[a, b]$, for any $ϵ > 0$,\n",
    "there exist parameters $w_i$, $b_i$, and $c_i$ such that:\n",
    "$$\n",
    "sup_{x∈[a,b]} E(x) < ϵ.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Thus, any continuous function on a compact interval $[a, b]$ can be approximated\n",
    "arbitrarily closely by a single-layer neural network with a ReLU activation\n",
    "function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## citations:\n",
    "An Overview Of Artificial Neural Networks for Mathematicians <br>\n",
    "by Leonardo Ferreira Guilhoto<br>\n",
    "https://math.uchicago.edu/~may/REU2018/REUPapers/Guilhoto.pdf\n",
    "\n",
    "Universal Approximation Theorem of sigmoid: A Rigorous Proof<br>\n",
    "provided by Idan Tobis<br>\n",
    "https://md.hit.ac.il/pluginfile.php/1035964/mod_resource/content/1/UAT.pdf"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
