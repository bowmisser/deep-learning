{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3f19443-dcc5-483e-ad2c-800081f5db16",
   "metadata": {},
   "source": [
    "# Homework #2: Simple Neural Network Implementation using Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "60f6cc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(suppress=True,precision=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704fb5c4-4f4e-41d5-adc9-95f97bb7950a",
   "metadata": {},
   "source": [
    "### Define the Sigmoid Function and its Derivative\n",
    "- Construct a function returning a sigmoid function:\n",
    "$ \\sigma(x) = \\frac{1}{1 + e^{-x}} $\n",
    "- Construct a function returning the derivative of a sigmoid function:\n",
    "$ \\frac{d\\sigma(x)}{dx} = \\sigma(x)(1 - \\sigma(x)) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "37eb31a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "#i did not define derivative of sigmiod and write 1-sigmoid instead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08594d28-1968-4ce6-bac7-4f42bed6ab41",
   "metadata": {},
   "source": [
    "### Initialize Weights\n",
    "Build an array of three weights (3x1 array â€“ think why these dimensions!) and initialize their value randomly. (It is good practice to use weights with normal distribution of $ \\mu = 0 $ and  $ \\sigma = \\frac{1}{3}  $ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8d9f3d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0, 0, 1],\n",
    "              [0, 1, 1],\n",
    "              [1, 0, 1],\n",
    "              [1, 1, 1]])\n",
    "y = np.array([[0],\n",
    "              [0],\n",
    "              [1],\n",
    "              [1]])\n",
    "weights = np.random.normal(0,1/3,4) #we want to have a (output layer, input layer + 1) matrix of weights (+1 because of bias)\n",
    "#in this case since the output layer has one node, we can use a vector of size 4 (3 nodes in the input layer + 1 bias)\n",
    "\n",
    "X = np.hstack((X,np.ones((X.shape[0],1)))) # we add ones at the end of X to account for bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc1001b",
   "metadata": {},
   "source": [
    "# we want to use gradiant decent<br>\n",
    "\n",
    "so we need to use the formula:\n",
    "$$\n",
    "\\theta _{t+1} = \\theta _k - \\alpha \\nabla L\n",
    "$$\n",
    "### we need to find $\\frac{dL}{dw^l_{ij}}$\n",
    "1. use chain rule\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w^l_{ij}} = \\frac{\\partial L}{\\partial O^l_j}\\frac{\\partial O^l_j}{\\partial w^l_{ij}}\\\\\n",
    "$$\n",
    "2. we need to find each derivative <br> <br>\n",
    "we will start with $\\frac{\\partial L}{\\partial O^l_i}$ (we assume $l$ is the output layer):\n",
    "$$ L = \\frac{1}{2}\\sum_i (O^l_i - y_i)^2 $$\n",
    "$$\\boxed{\\frac{\\partial L}{\\partial O^l_i} = (O^l_i - y_i)}$$\n",
    "\n",
    "now we will find $\\frac{\\partial O^l_i}{\\partial w^l_{ij}}$:\n",
    "\n",
    "$$z^l_j = \\sum_i w^l_{ij}O^{l-1}_{i}$$\n",
    "$$O^l_j = \\sigma(\\sum_i w^l_{ij}O^{l-1}_i) = \\sigma(z^l_j)$$\n",
    "$$\\frac{\\partial O^l_j}{\\partial w^l_{ij}} = \\frac{\\partial O^l_j}{\\partial z^l_{j}}\\frac{\\partial z^l_{j}}{\\partial w^l_{ij}}$$\n",
    "$$\\frac{\\partial O^l_j}{\\partial z^l_{j}} = \\sigma'(z^l_{j})\\quad \\quad\n",
    "\\frac{\\partial z^l_{j}}{\\partial w^l_{ij}} = O^{l-1}_{i}$$\n",
    "$${\\big \\Downarrow}$$\n",
    "$$\\boxed{\\frac{\\partial O^l_j}{\\partial w^l_{ij}} = O^{l-1}_{i}\\sigma'(z^l_{j})}$$\n",
    "now we just combine them together to get:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w^l_{ij}} = \\frac{\\partial L}{\\partial O^l_j}\\frac{\\partial O^l_j}{\\partial w^l_{ij}} = (O^l_j - y_i)(O^{l-1}_{i}\\sigma'(z^l_{j})) = O^{l-1}_{i}(O^l_j - y_i)\\sigma(z^l_{j})(1-\\sigma(z^l_{j}))$$\n",
    "$$\\boxed{\\frac{\\partial L}{\\partial w^l_{ij}} = O^{l-1}_{i}(O^l_j - y_i)\\sigma(z^l_{j})(1-\\sigma(z^l_{j}))}\n",
    "$$\n",
    "\n",
    "notice:\n",
    "we have 0 hidden layers so $l = 2$ <br>\n",
    "we also define $O^1_4 = 1$ which means it defines the bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d47ab35-6fe8-4fe2-bdda-85d970e810e3",
   "metadata": {},
   "source": [
    "### Training the Neural Network\n",
    "Create a loop, iterating 1000 times (equal to the desired number of learning steps). For each iteration, calculate the difference between the network prediction and the real value of y. Multiply that difference with the sigmoid derivative and use the dot product of this number with the input layer to update your weights for the next iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ab6f81ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(X,weights):\n",
    "    return sigmoid(X @ weights)\n",
    "def loss(y_pred,y_true):\n",
    "    return 0.5*np.sum((y_pred-y_true[:,0])**2)\n",
    "def train(X,y,weights,epoch=1000,learning_rate=1):\n",
    "    print(\"starting training:\")\n",
    "    for i in range(epoch):\n",
    "        y_pred = prediction(X,weights)\n",
    "        sigmoid_z = y_pred\n",
    "        dL_dw = X*(y_pred-y[:,0])*sigmoid_z*(1-sigmoid_z)\n",
    "        weights = weights - learning_rate*np.sum(dL_dw,axis=1)\n",
    "        if (i % 200 == 0):\n",
    "            print(\"epoch =\",i)\n",
    "            print(\"Loss =\",loss(y_pred,y))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "426a8e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training:\n",
      "epoch = 0\n",
      "Loss = 0.674335077116371\n",
      "epoch = 200\n",
      "Loss = 0.009084894139190383\n",
      "epoch = 400\n",
      "Loss = 0.004161735796729394\n",
      "epoch = 600\n",
      "Loss = 0.0026720308653233583\n",
      "epoch = 800\n",
      "Loss = 0.0019604128874348603\n",
      "-----------\n",
      "trained model:\n",
      "Loss = 0.001545275635414038\n",
      "prediction = [0.02892 0.03373 0.97462 0.97827]\n",
      "ground truth = [0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "model = train(X,y,weights) \n",
    "print(\"-----------\")\n",
    "print(\"trained model:\")  \n",
    "y_pred = prediction(X,model)\n",
    "\n",
    "print('Loss =',loss(y_pred,y))\n",
    "print(\"prediction =\",y_pred)\n",
    "print(\"ground truth =\",y[:,0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
